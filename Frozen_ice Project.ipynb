{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc81ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1569e360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake\",render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdb4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "\n",
    "\n",
    "q_table=np.zeros((state_space_size,action_space_size))\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef8cfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=10000\n",
    "max_steps_per_episode=100\n",
    "\n",
    "learning_rate=0.1\n",
    "discount_rate=0.99\n",
    "\n",
    "exploration_rate=1\n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743f4996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Average reward per thousand episodes*****\n",
      "\n",
      "1000  :  0.03400000000000002\n",
      "2000  :  0.21400000000000016\n",
      "3000  :  0.4290000000000003\n",
      "4000  :  0.5590000000000004\n",
      "5000  :  0.6520000000000005\n",
      "6000  :  0.6590000000000005\n",
      "7000  :  0.6720000000000005\n",
      "8000  :  0.6680000000000005\n",
      "9000  :  0.6780000000000005\n",
      "10000  :  0.6920000000000005\n",
      "\n",
      "\n",
      " ******** Q-table***********\n",
      "\n",
      "[[0.58799275 0.51176071 0.50889236 0.51707248]\n",
      " [0.3970355  0.35846067 0.36559446 0.53529729]\n",
      " [0.40890404 0.42196224 0.40336408 0.47980876]\n",
      " [0.36911404 0.30543768 0.34007224 0.45391466]\n",
      " [0.59835968 0.39186241 0.4097385  0.24754623]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20849826 0.08149904 0.32991197 0.20326021]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44230122 0.34149624 0.3289257  0.62457904]\n",
      " [0.33828807 0.66074664 0.51476001 0.37352293]\n",
      " [0.6635768  0.39234057 0.40245743 0.404595  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34718703 0.6133203  0.78704023 0.47708646]\n",
      " [0.73731545 0.92345512 0.7790606  0.73622808]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes=[]\n",
    "\n",
    "#Q-Learning algorithm\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "  state=env.reset()[0]\n",
    "  done=False\n",
    "  rewards_current_episode=0\n",
    "\n",
    "  for step in range (max_steps_per_episode):\n",
    "    # Exploration-exploitation trade off\n",
    "    exploration_rate_threshold=random.uniform(0,1)\n",
    "    if exploration_rate_threshold>exploration_rate:\n",
    "       action=np.argmax(q_table[state,:])\n",
    "    else :\n",
    "       action =env.action_space.sample()\n",
    "\n",
    "    new_state,reward,done,truncated,info=env.step(action)\n",
    "\n",
    "\n",
    "    \n",
    "#Update Q-table for Q(s,a)\n",
    "\n",
    "    q_table[state,action]=q_table[state,action]*(1-learning_rate)+\\\n",
    "    learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))\n",
    "\n",
    "    state=new_state\n",
    "\n",
    "    rewards_current_episode += reward\n",
    "    if done == True:\n",
    "      break\n",
    "\n",
    "\n",
    "#Exploration rate decay\n",
    "  \n",
    "  exploration_rate=min_exploration_rate +\\\n",
    "     (max_exploration_rate-min_exploration_rate)*\\\n",
    "     np.exp(-exploration_decay_rate*episode)\n",
    "  \n",
    "  rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "#Calculate and print the average reward per thousand episodes\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),\\\n",
    "                                         num_episodes/1000)\n",
    "count=1000\n",
    "\n",
    "print(\"*****Average reward per thousand episodes*****\\n\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "  print(count,\" : \",str(sum(r/1000)))\n",
    "  count+=1000\n",
    "\n",
    "\n",
    "#print updated Q-table\n",
    "\n",
    "print(\"\\n\\n ******** Q-table***********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e61ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****You reached the goal!!*****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "  state=env.reset()[0]\n",
    "  done = False\n",
    "  print(\"*****Episode \" , episode+1 , \"*****\\n\\n\\n\\n\")\n",
    "  time.sleep(1)\n",
    "  for step in range (max_steps_per_episode):\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    action=np.argmax(q_table[state,:])\n",
    "    new_state,reward,done,truncated,info=env.step(action)\n",
    "\n",
    "    if done:\n",
    "      clear_output(wait=True)\n",
    "      env.render()\n",
    "      if reward==1:\n",
    "        print(\"****You reached the goal!!*****\")\n",
    "        time.sleep(3).\n",
    "        \n",
    "      else:\n",
    "        print(\"****You fell through a hole!!****\")\n",
    "        time.sleep(3)\n",
    "      clear_output(wait=True)\n",
    "      break\n",
    "    state=new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df15b4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
